{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca5d00ba",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 姚瀚宇\n",
    "\n",
    "Student ID: 110062542\n",
    "\n",
    "GitHub ID: 51129597"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d9e83",
   "metadata": {},
   "source": [
    "## 4. Improve the data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681062b",
   "metadata": {},
   "source": [
    "#### First, there are some small issues regarding to coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83f1c6",
   "metadata": {},
   "source": [
    "1. In 3.1 Converting Dictionary into Pandas Dataframe, there is a cell adding category_name column to the DataFrame. This cell calls the function `format_labels(target, docs)` from data_mining_helpers.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dcf443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add category label also\n",
    "X['category_name'] = X.category.apply(lambda t: dmh.format_labels(t, twenty_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c89389",
   "metadata": {},
   "source": [
    "However, after taking a look at the function `format_labels(target, docs)`, I think that this function is redundant. The cell can directly be written as below without calling the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a695fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add category label also\n",
    "X['category_name'] = X.category.apply(lambda t: twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234c6c8",
   "metadata": {},
   "source": [
    "---\n",
    "2. In 5.5 Atrribute Transformation / Aggregation, there is one cell calculating the term frequencies and it takes some time to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd5a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this takes time to compute. You may want to reduce the amount of terms you want to compute frequencies for\n",
    "term_frequencies = []\n",
    "for j in range(0,X_counts.shape[1]):\n",
    "    term_frequencies.append(sum(X_counts[:,j].toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9aab00",
   "metadata": {},
   "source": [
    "However, the efficient way to conduct the same process is listed right below this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b913bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequencies = np.asarray(X_counts.sum(axis=0))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4ec13",
   "metadata": {},
   "source": [
    "These two cells are performing the same thing (calculating the term frequencies), but the second one performs more efficiently. Therefore, we only need to keep the second cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9033631",
   "metadata": {},
   "source": [
    "---\n",
    "3. In 6. Data Exploration, there is one cell retrieving sentences from the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve 2 sentences for a random record, here, indexed at 50 and 100\n",
    "document_to_transform_1 = []\n",
    "random_record_1 = X.iloc[50]\n",
    "random_record_1 = random_record_1['text']\n",
    "document_to_transform_1.append(random_record_1)\n",
    "\n",
    "document_to_transform_2 = []\n",
    "random_record_2 = X.iloc[100]\n",
    "random_record_2 = random_record_2['text']\n",
    "document_to_transform_2.append(random_record_2)\n",
    "\n",
    "document_to_transform_3 = []\n",
    "random_record_3 = X.iloc[150]\n",
    "random_record_3 = random_record_3['text']\n",
    "document_to_transform_3.append(random_record_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5544da2a",
   "metadata": {},
   "source": [
    "Nevertheless, I think that this cell can be written more concisely as below three lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_to_transform_1 = [X['text'][50]]\n",
    "document_to_transform_2 = [X['text'][100]]\n",
    "document_to_transform_3 = [X['text'][150]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa7fd02",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd789e7",
   "metadata": {},
   "source": [
    "#### Second, when preprocessing text data, we can do something to make the data cleaner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea6cb2",
   "metadata": {},
   "source": [
    "1. Clean the text data by removing punctuation marks and converting all characters to lowercase. (implemented by re)\n",
    "2. Perform word stemming (implemented by nltk) on text data, which grammatically transforms words back to root form. In this way, we can treat all the different form words as the same when calculating frequency or TF-IDF.\n",
    "3. Remove stop-words (implemented by nltk), which are words that are extremely common in all sorts of texts thus contain little useful information that can be used to distinguish between different classes of documents. For example, \"is\", \"and\", \"has\", and \"the\".\n",
    "4. Remove numbers if they are not relevant to the analysis. (implemented by re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d34a84",
   "metadata": {},
   "source": [
    "Use the new dataset as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "495e0004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 3000\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "text = []\n",
    "score = []\n",
    "files = ['amazon_cells_labelled.txt', 'imdb_labelled.txt', 'yelp_labelled.txt']\n",
    "\n",
    "for file in files:\n",
    "    f = open('sentiment labelled sentences/' + file)\n",
    "    for line in f:\n",
    "        line_lst = line.split('\\t')\n",
    "        text.append(line_lst[0])\n",
    "        score.append(int(line_lst[1][0]))\n",
    "\n",
    "print(len(text), len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5196d254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "dic = {\"text\":text, \"score\":score}\n",
    "X = pd.DataFrame(dic)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d70f4f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated records\n",
    "X.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1884638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation marks but emoticons, and converting all characters to lowercase\n",
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "X['text'] = X['text'].apply(lambda t : preprocessor(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bfd97cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word stemming\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return \" \".join([porter.stem(word) for word in re.split('\\s+', text.strip())])\n",
    "\n",
    "X['text'] = X['text'].apply(lambda t : tokenizer_stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5851e86d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/yao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stop-word removal\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return \" \".join([porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "    if w not in stop and re.match('[a-zA-Z]+', w)])\n",
    "\n",
    "X['text'] = X['text'].apply(lambda t : tokenizer_stem_nostop(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beeb9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the term-document matrix for frequency\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "doc_counts = count_vect.fit_transform(X.text)\n",
    "doc_counts = doc_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c295459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer and fit_transform it to X.text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_tfidf = vectorizer.fit_transform(X.text)\n",
    "doc_tfidf = doc_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c983d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2983, 7750)\n"
     ]
    }
   ],
   "source": [
    "# combine frequency feature and tf-idf feature together\n",
    "XX = np.append(doc_counts, doc_tfidf, axis=1)\n",
    "print(XX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00b0132c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 597 points : 107\n",
      "Accuracy: 82.08%\n",
      "Number of mislabeled points out of a total 597 points : 116\n",
      "Accuracy: 80.57%\n",
      "Number of mislabeled points out of a total 597 points : 107\n",
      "Accuracy: 82.08%\n",
      "Number of mislabeled points out of a total 597 points : 114\n",
      "Accuracy: 80.90%\n",
      "Number of mislabeled points out of a total 597 points : 112\n",
      "Accuracy: 81.24%\n",
      "Number of mislabeled points out of a total 597 points : 116\n",
      "Accuracy: 80.57%\n",
      "Number of mislabeled points out of a total 597 points : 123\n",
      "Accuracy: 79.40%\n",
      "Number of mislabeled points out of a total 597 points : 106\n",
      "Accuracy: 82.24%\n",
      "Number of mislabeled points out of a total 597 points : 108\n",
      "Accuracy: 81.91%\n",
      "Number of mislabeled points out of a total 597 points : 103\n",
      "Accuracy: 82.75%\n"
     ]
    }
   ],
   "source": [
    "# Multinomial naive Bayes classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "acc_mnb = []\n",
    "y = X['score']\n",
    "\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.2)\n",
    "    mnb = MultinomialNB(alpha=1.0)\n",
    "    y_pred = mnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "    total_test_data = X_test.shape[0]\n",
    "    wrong_test_data = (y_test != y_pred).sum()\n",
    "    correct_test_data = (y_test == y_pred).sum()\n",
    "    print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "          % (total_test_data, wrong_test_data))\n",
    "    print(\"Accuracy: {:.2f}%\".format(correct_test_data/total_test_data*100))\n",
    "    acc_mnb.append(correct_test_data/total_test_data*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "293a277d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy of Multinomial naive Bayes classifier: 81.37%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy of Multinomial naive Bayes classifier: {:.2f}%\".format(sum(acc_mnb)/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8e55fc",
   "metadata": {},
   "source": [
    "However, after performing these text data preprocessing techniques, the accuracy is nearly the same as previous accuracy (without these text data preprocessing). Maybe some more feature engineering techniques can be applied to this dataset rather than using only frequency and TF-IDF score as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766a0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
